import numpy as np
import os

# ==============================================================================
# 1. ATTRIBUTE FILES (Generated by generate_missing_files.py)
# ==============================================================================
USER_ACTIVENESS_PATH = "user_activeness.npy"
USER_MAINSTREAM_PATH = "user_mainstream.npy"
ITEM_POPULARITY_PATH = "item_popularity.npy"
ITEM_MAINSTREAM_PATH = "item_mainstream.npy"

# ==============================================================================
# 2. MODEL RESULTS CONFIGURATION
# ==============================================================================
# Structure: "Model Name": { "user": "PATH_TO_USER_SCORES", "item": "PATH_TO_ITEM_SCORES" }
# IMPORTANT: You must update the paths below with your actual filenames!
# ==============================================================================

# windows = back \
# linux = forward /
MODEL_FILES = {
    "MF (Baseline)": {
        "user": r"data/recsys_data/amazon_cds/bias_scores/MF_scores/20260114-1514_mf_scores.npy",
        "item": r"data/recsys_data/amazon_cds/bias_scores/MF_scores/20260114-1514_ITEM_scores.npy" 
    },
    "CFAdaBoost (Design 1)": {
        "user": r"data/recsys_data/amazon_cds/bias_scores/MF_CFadaboost_scores/20260114-1526_boost_scores.npy",
        "item": r"data/recsys_data/amazon_cds/bias_scores/MF_CFadaboost_scores/20260114-1526_ITEM_scores.npy"
    },
    "CFBoost (Design 2)": {
        "user": r"data/recsys_data/amazon_cds/bias_scores/MF_adaboost_scores/20260114-1515_boost_scores.npy",
        "item": r"data/recsys_data/amazon_cds/bias_scores/MF_adaboost_scores/20260114-1515_ITEM_scores.npy"
    }
}

# ==============================================================================
# CORE LOGIC
# ==============================================================================

def load_user_ndcg(file_path):
    """Loads NDCG@20 from the user score dictionary."""
    if not os.path.exists(file_path): return None
    try:
        data = np.load(file_path, allow_pickle=True)
        if data.shape == (): data = data.item()
        # Find NDCG@20 key
        keys = list(data.keys())
        target_key = next((k for k in keys if "NDCG" in k and "20" in k), None)
        return np.array(data[target_key]) if target_key else None
    except: return None

def load_item_mdg(file_path):
    """Loads raw MDG scores from the item score array."""
    if not os.path.exists(file_path): return None
    try:
        data = np.load(file_path, allow_pickle=True)
        return data # It's already a flat array of scores
    except: return None

def generate_table(table_name, attribute_path, models_dict, metric_type="user"):
    """
    Generic function to generate bias tables.
    metric_type: "user" (for Tables 1 & 2) or "item" (for Tables 3 & 4)
    """
    print(f"\n{'='*80}")
    print(f"REPRODUCING {table_name}")
    print(f"{'='*80}")

    # 1. Load Attribute Data
    if not os.path.exists(attribute_path):
        print(f"[Error] Attribute file not found: {attribute_path}")
        return

    attr_scores = np.load(attribute_path, allow_pickle=True)
    if len(attr_scores.shape) > 1: attr_scores = attr_scores.flatten()

    # 2. Sort Indices (Low -> High)
    sorted_indices = np.argsort(attr_scores)
    
    # 3. Split into 5 Groups
    grouped_indices = np.array_split(sorted_indices, 5)
    group_names = ["Low", "Med-Low", "Medium", "Med-High", "High"]

    # 4. Print Header
    header = f"{'Model':<25} | " + " | ".join([f"{name:<10}" for name in group_names])
    print(header)
    print("-" * 90)

    # 5. Process Each Model
    for model_name, paths in models_dict.items():
        score_path = paths.get(metric_type)
        
        # Load appropriate data based on type
        if metric_type == "user":
            scores = load_user_ndcg(score_path)
        else:
            scores = load_item_mdg(score_path)

        if scores is None:
            print(f"{model_name:<25} | [FILE MISSING] - Check path in script config")
            continue

        # Sanity Check
        if len(scores) != len(attr_scores):
            # This handles cases where data sizes might mismatch slightly due to preprocessing
            # We truncate to minimum length to prevent crash, though usually they match exactly
            min_len = min(len(scores), len(attr_scores))
            # print(f"[Warning] Size mismatch for {model_name}: Attr {len(attr_scores)} vs Score {len(scores)}")
            # Skip if severe mismatch
            if abs(len(scores) - len(attr_scores)) > 1000:
                print(f"{model_name:<25} | [SIZE ERROR] Data mismatch")
                continue

        # Calculate Group Averages
        row_str = f"{model_name:<25} | "
        for group_idx in grouped_indices:
            # Filter indices that are within bounds of the score array
            valid_idx = group_idx[group_idx < len(scores)]
            
            if len(valid_idx) == 0:
                avg = 0.0
            else:
                # Extract scores for this group
                group_vals = scores[valid_idx]
                # Remove NaNs (important for Item scores where some items have 0 interactions)
                group_vals = group_vals[~np.isnan(group_vals)]
                avg = np.mean(group_vals) if len(group_vals) > 0 else 0.0
            
            row_str += f"{avg:.4f}     | "
        print(row_str)

if __name__ == "__main__":
    # ==========================================
    # USER BIAS TABLES (Metric: NDCG@20)
    # ==========================================
    generate_table("TABLE 1: USER MAINSTREAM BIAS", USER_MAINSTREAM_PATH, MODEL_FILES, metric_type="user")
    generate_table("TABLE 2: USER ACTIVENESS BIAS", USER_ACTIVENESS_PATH, MODEL_FILES, metric_type="user")

    # ==========================================
    # ITEM BIAS TABLES (Metric: MDG@20)
    # ==========================================
    generate_table("TABLE 3: ITEM MAINSTREAM BIAS", ITEM_MAINSTREAM_PATH, MODEL_FILES, metric_type="item")
    generate_table("TABLE 4: ITEM POPULARITY BIAS", ITEM_POPULARITY_PATH, MODEL_FILES, metric_type="item")